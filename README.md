# AgentDog

**AgentDog** is a risk-aware evaluation and guarding framework for autonomous agents. It focuses on *trajectory-level risk assessment*, aiming to determine whether an agentâ€™s execution trajectory contains safety risks under diverse application scenarios.

---

## ğŸ” Overview

Autonomous agents (e.g., tool-using LLM agents, mobile agents, web agents) often execute multi-step trajectories consisting of observations, reasoning, and actions. Existing safety mechanisms mainly focus on **single-step content moderation** or **final-output filtering**, which are insufficient for capturing risks emerging *during execution*.

**AgentDog** addresses this gap by providing **trajectory-level safety assessment** that monitors the entire execution process, not just final outputs.

AgentDog can be used as:

* A **benchmark** for agent safety evaluation
* A **risk classifier** for agent trajectories
* A **guard module** integrated into agent systems

| Name | Type | Download |
|------|------|----------|
| AgentDog-Gen-0.6B | Generative Guard | ğŸ¤— [Hugging Face](https://huggingface.co/AI45Research/AgentGuard) |
| AgentDog-Gen-4B | Generative Guard | ğŸ¤— [Hugging Face](https://huggingface.co/AI45Research/AgentGuard) |
| AgentDog-Gen-8B | Generative Guard | ğŸ¤— [Hugging Face](https://huggingface.co/AI45Research/AgentGuard) |
| AgentDog-Stream-0.6B | Stream Guard | ğŸ¤— [Hugging Face](https://huggingface.co/AI45Research/AgentGuard) |

For more details, please refer to our [Technical Report](https://github.com//blob/main/_Technical_Report.pdf).

---

## âœ¨ Safety Taxonomy
We adopt a unified, three-dimensional safety taxonomy for agentic systems. It organizes risks along three orthogonal axes, answering: why a risk arises (risk source), how it manifests in behavior (failure mode), and what harm it causes(risk consequence).

* **Risk Source**: where the threat originates in the agent loop, e.g., user inputs, environmental observations,
  external tools/APIs, or the agent's internal reasoning.
* **Failure Mode**: how the unsafe behavior is realized, such as flawed planning, unsafe tool usage,
  instruction-priority confusion, or unsafe content generation.
* **Risk Consequence**: the real-world impact, including privacy leakage, financial loss, physical harm,
  security compromise, or broader societal/psychological harms.

In the current release, the taxonomy includes 8 risk-source categories, 14 failure modes, and 10 risk-consequence categories, and is used for fine-grained labeling during training and evaluation.

---

## ğŸ§  Methodology

### Task Definition

<p align="center">
  <img src="figures/agentguard_eg.png" width="80%" alt="AgentDog Task Example"/>
</p>
<p align="center"><em>Figure: Example task input for AgentDog. The model analyzes the full trajectory and outputs a safety label with fine-grained categorization.</em></p>

Prior works (e.g., LlamaGuard, Qwen3Guard) formulate safety moderation as classifying whether the **final output** in a multi-turn chat is safe. In contrast, **AgentDog** defines a different task: **diagnosing an entire agent trajectory** to determine whether the agent exhibits any unsafe behavior at **any point** during execution.

The input prompt consists of four components:

| Component | Description |
|-----------|-------------|
| **Task Definition** | Instructs the model to evaluate the trajectory for unsafe actions |
| **Safety Taxonomy** | Provides risk sources/triggers and failure modes as an analytical framework |
| **Agent Trajectory** | The multi-turn execution trace including user, agent, and environment interactions |
| **Output Specification** | Requires the model to output `safe` or `unsafe`, with fine-grained subtype labels for unsafe cases |

###  Data Synthesis and Collection

We use a **taxonomy-guided** synthesis pipeline to generate realistic, multi-step agent trajectories. Each trajectory is conditioned on a sampled risk tuple (risk source, failure mode, risk consequence), then expanded into a coherent tool-augmented execution and filtered by quality checks.

<p align="center">
  <img src="figures/data_synthesis_main.png" width="95%" alt="Data Synthesis Pipeline"/>
</p>
<p align="center"><em>Figure: Three-stage pipeline for multi-step agent safety trajectory synthesis.</em></p>

To reflect realistic agent tool use, our tool library is orders of magnitude larger than prior benchmarks. For example, it is about 86x, 55x, and 41x larger than R-Judge, ASSE-Safety, and ASSE-Security, respectively.

<p align="center">
  <img src="figures/tool_comparison.png" width="90%" alt="Tool library size comparison"/>
</p>
<p align="center"><em>Figure: Tool library size compared to existing agent safety benchmarks.</em></p>

We also track the coverage of the three taxonomy dimensions (risk source, failure mode, and harm type) to ensure balanced and diverse risk distributions in our synthesized data.

<p align="center">
  <img src="figures/distribution_comparison.png" width="90%" alt="Taxonomy distribution comparison"/>
</p>
<p align="center"><em>Figure: Distribution over risk source, failure mode, and harm type categories.</em></p>

### Training


## ğŸ“Š Performance Highlights

* Evaluated on **R-Judge**, **ASSE-Safety**, and **GooDoG**
* Outperforms step-level baselines in detecting:

  * Long-horizon instruction hijacking
  * Tool misuse after benign prefixes
* Strong generalization across:

  * Different agent frameworks
  * Different LLM backbones
* Fine-grained accuracy on GooDoG: Risk Source **80.8%**, Harm Type **60.0%**

Accuracy comparison (ours + baselines):

| Model                          | Type          | R-Judge | ASSE-Safety | GooDoG |
| ----------------------------- | ------------- | ------- | ----------- | ------ |
| **AgentDog (Ours)**           | Guard         | **90.4** | **81.1**    | **85.6** |
| GPT-5.2                       | General       | 83.5    | 79.5        | 89.2   |
| Gemini-3-Flash                | General       | 93.6    | 81.5        | 94.4   |
| Gemini-3-Pro                  | General       | 93.8    | 81.1        | 95.2   |
| QwQ-32B                       | General       | 86.9    | 75.3        | 80.2   |
| Qwen3-235B-A22B-Instruct       | General       | 75.5    | 79.0        | 91.0   |
| LlamaGuard3-8B                | Guard         | 61.2    | 54.5        | 53.3   |
| LlamaGuard4-12B               | Guard         | 63.8    | 56.3        | 58.1   |
| Qwen3-Guard                   | Guard         | 40.6    | 48.2        | 55.3   |
| ShieldAgent                   | Guard         | 81.0    | 79.6        | 76.0   |

---

## ğŸš€ Getting Started

### Installation

```bash
git clone https://github.com/your-org/AgentGuard.git
cd AgentGuard
pip install -r requirements.txt
```

### Basic Usage

```python
from agentguard import Guard

guard = Guard(model="agentguard-base")
result = guard.evaluate(trajectory)

print(result.is_unsafe)
print(result.risk_type)
print(result.failure_mode)
```

---

## ğŸ§ª Evaluation

To reproduce experiments:

```bash
python scripts/evaluate.py \
  --dataset r-judge \
  --model agentguard-base
```

Supported datasets:

* R-Judge
* ASSE-Safety
* GooDoG (ToolBench)

---

## Agentic XAI Attribution
We also introduce a novel hierarchical framework for Agentic Attribution, designed to unveil the internal drivers behind agent actions beyond simple failure localization. By decomposing interaction trajectories into pivotal components and fine-grained textual evidence, our approach explains why an agent makes specific decisions regardless of the outcome. This framework enhances the transparency and accountability of autonomous systems by identifying key factors such as memory biases and tool outputs.
### Case Study
To evaluate the effectiveness of the proposed agentic attribution framework, we conducted several case studies across diverse scenarios. The figure illustrates how our framework localizes decision drivers across four representative cases. The highlighted regions denote the historical components and fine-grained sentences identified by our framework as the primary decision drivers.

<p align="center">
  <img src="figures/agent_xai_fig_exp_case.jpg" width="95%" alt="Data Synthesis Pipeline"/>
</p>
<p align="center"><em>Figure: Illustration of attribution results across four representative scenarios.</em></p>



### Quick Start
#### Data Preparation
Ensure your input data is a JSON file containing a trajectory (or trace) list:
```json
{
  "trajectory": [
    {"role": "system", "content": "System prompt..."},
    {"role": "user", "content": "User query..."},
    {"role": "assistant", "content": "Agent response..."},
    {"role": "tool", "content": "Tool output..."}
  ]
}
```
#### Run Analysis Pipeline
You can run the analysis in three steps:

##### Step 1: Trajectory-Level Attribution \
Analyze the contribution of each conversation step.
```bash
python component_attri.py \
  --model_id meta-llama/Meta-Llama-3.1-70B-Instruct \
  --data_dir ./data \
  --output_dir ./results
```
##### Step 2: Sentence-Level Attribution \
Perform fine-grained analysis on the top-K most influential steps.
```bash
python sentence_attri.py \
  --model_id meta-llama/Meta-Llama-3.1-70B-Instruct \
  --attr_file ./results/case1_attr.json \
  --traj_file ./data/case1.json \
  --top_k 3
```


##### Step 3: Generate Visualization \
Create an interactive HTML heatmap.
```bash
python case_plot_html.py \
  --traj_attr_file ./results/case_attr.json \
  --sent_attr_file ./results/case_attr_sentence.json \
  --original_traj_file ./data/case.json \
  --output_file ./results/visualization.html
```

##### One-Click Execution \
To run the complete pipeline automatically, configure and run the shell script:
```bash
bash run_all_pipeline.sh
```


## ğŸ“ Repository Structure

```
AgentGuard/
â”œâ”€â”€ agentguard/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ taxonomy/
â”‚   â””â”€â”€ guard.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ preprocess.py
â”œâ”€â”€ benchmarks/
â”œâ”€â”€ README.md
â””â”€â”€requirements.txt
```

---

## ğŸ› ï¸ Customization

* **Add new risk types**: `taxonomy/`
* **Support new agent formats**: implement a trajectory parser
* **Plug in new models**: follow `models/base.py`

---

## ğŸ“œ License

This project is released under the **Apache 2.0 License**.

---

## ğŸ“– Citation

If you use AgentGuard in your research, please cite:

```bibtex
@article{,
  title={AgentGuard: Trajectory-Level Risk Assessment for Autonomous Agents},
  author={Anonymous},
  year={2025}
}
```

---

## ğŸ¤ Acknowledgements

This project builds upon prior work in agent safety, trajectory evaluation, and risk-aware AI systems.
