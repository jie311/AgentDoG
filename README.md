# AgentGuard

**AgentGuard** is a risk-aware evaluation and guarding framework for autonomous agents. It focuses on *trajectory-level risk assessment*, aiming to determine whether an agentâ€™s execution trajectory contains safety risks under diverse application scenarios.

---

## ğŸ” Overview

Autonomous agents (e.g., tool-using LLM agents, mobile agents, web agents) often execute multi-step trajectories consisting of observations, reasoning, and actions. Existing safety mechanisms mainly focus on **single-step content moderation** or **final-output filtering**, which are insufficient for capturing risks emerging *during execution*.

**AgentGuard** addresses this gap by:



AgentGuard can be used as:

* A **benchmark** for agent safety evaluation
* A **risk classifier** for agent trajectories
* A **guard module** integrated into agent systems

| Name                  | Type     |Download                                                                                                                                                                        |
|-----------------------------|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| AgentGuard-0.6B         | Generative Guard    | ğŸ¤— [Hugging Face](https://huggingface.co/AI45Research/AgentGuard  )                                |
| AgentGuard-Gen-4B         | Generative Guard     | ğŸ¤— [Hugging Face](https://huggingface.co/AI45Research/AgentGuard  )                                           |
| AgentGuard-8B         | Generative Guard     | ğŸ¤— [Hugging Face](https://huggingface.co/AI45Research/AgentGuard  )                              |
|AgentGuard-Stream-0.6B         | Stream Guard     | ğŸ¤— [Hugging Face](https://huggingface.co/AI45Research/AgentGuard  )                               |


For more details, please refer to our [Technical Report](https://github.com//blob/main/_Technical_Report.pdf).
---

## âœ¨ Safety Taxonomy


---

## ğŸ§  Methodology

### Task Definition


###  Data Synthesis and Collection


### Training


## ğŸ“Š Performance Highlights

> *(Fill in with your experimental results)*

* Evaluated on **AgentJudge**, **SAFE**, and internal benchmarks
* Outperforms step-level baselines in detecting:

  * Long-horizon instruction hijacking
  * Tool misuse after benign prefixes
* Strong generalization across:

  * Different agent frameworks
  * Different LLM backbones

Example metrics:

| Dataset    | Accuracy | F1 (Unsafe) | AUROC |
| ---------- | -------- | ----------- | ----- |
| AgentJudge | XX.X     | XX.X        | XX.X  |
| SAFE       | XX.X     | XX.X        | XX.X  |

---

## ğŸš€ Getting Started

### Installation

```bash
git clone https://github.com/your-org/AgentGuard.git
cd AgentGuard
pip install -r requirements.txt
```

### Basic Usage

```python
from agentguard import Guard

guard = Guard(model="agentguard-base")
result = guard.evaluate(trajectory)

print(result.is_unsafe)
print(result.risk_type)
print(result.failure_mode)
```

---

## ğŸ§ª Evaluation

To reproduce experiments:

```bash
python scripts/evaluate.py \
  --dataset agentjudge \
  --model agentguard-base
```

Supported datasets:

* AgentJudge
* SAFE
* Custom JSON trajectories

---


## ğŸ“ Repository Structure

```
AgentGuard/
â”œâ”€â”€ agentguard/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ taxonomy/
â”‚   â””â”€â”€ guard.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ preprocess.py
â”œâ”€â”€ benchmarks/
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ› ï¸ Customization

* **Add new risk types**: `taxonomy/`
* **Support new agent formats**: implement a trajectory parser
* **Plug in new models**: follow `models/base.py`

---

## ğŸ“œ License

This project is released under the **Apache 2.0 License**.

---

## ğŸ“– Citation

If you use AgentGuard in your research, please cite:

```bibtex
@article{,
  title={AgentGuard: Trajectory-Level Risk Assessment for Autonomous Agents},
  author={Anonymous},
  year={2025}
}
```

---

## ğŸ¤ Acknowledgements

This project builds upon prior work in agent safety, trajectory evaluation, and risk-aware AI systems.
